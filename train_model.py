# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Systematicos/Model-dectation-fake-news/blob/v4/notebooks/main.ipynb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
import re
import string
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report, accuracy_score
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.model_selection import train_test_split

import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, Conv1D, Dense, Dropout, Attention, Bidirectional
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import plot_model
import os
import nltk
nltk.download('stopwords')
log_dir = "logs/"  # Especifique o diretório onde os logs serão armazenados
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

df = pd.read_csv('data/fakeBr.csv')
df = df.drop(columns=['index'])
df.isnull().any()

stop_words = set(stopwords.words('portuguese'))

def remover_stop_words(news):
    palavras = news.split()
    palavras_sem_stop = [palavra for palavra in palavras if palavra.lower() not in stop_words]
    return ' '.join(palavras_sem_stop)

def review_cleaning(text):

    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

df["preprocessed_news"] = df["preprocessed_news"].apply(remover_stop_words)
df['label'] = df.apply(lambda row: 0 if row.label == 'fake' else 1, axis=1)
X = df.drop(['label'], axis = 1)
Y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify=Y)
X_train = X_train['preprocessed_news'].apply(lambda x: x.lower())
X_test = X_test['preprocessed_news'].apply(lambda x: x.lower())

#variaveis dos modelos
maxlen=256
num_words = 8000
batch_size = 128
epochs = 20
validation_fraction = 0.2
output_dim = 64



early_stopping = EarlyStopping(monitor='val_loss', patience=3)

train_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>',num_words=num_words)
train_tokenizer.fit_on_texts(X_train.values)
train_word_index = train_tokenizer.word_index
train_sequences = train_tokenizer.texts_to_sequences(X_train)

text_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>',num_words=num_words)
text_tokenizer.fit_on_texts(X_test.values)
text_word_index = text_tokenizer.word_index
text_sequences = text_tokenizer.texts_to_sequences(X_test)

vocab_length = len(train_word_index) + 1

train_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen)
test_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(text_sequences, maxlen=maxlen)

train_padded_seqeunces = train_padded_seqeunces[:, :, tf.newaxis]
test_padded_seqeunces = test_padded_seqeunces[:, :, tf.newaxis]

x_train_padded_seqeunces = train_padded_seqeunces[:, :, tf.newaxis]
x_test_padded_seqeunces = test_padded_seqeunces[:, :, tf.newaxis]

vectorizer = CountVectorizer(max_features=num_words)
vectorizer.fit(X_train)
X_train = vectorizer.transform(X_train).toarray()

vectorizer = CountVectorizer(max_features=num_words)
vectorizer.fit(X_test)
X_test = vectorizer.transform(X_test).toarray()

"""#### MLP"""

print("=" * 20)
print("Modelo Multi Layers Perceptron")

parameters = {'solver': ['sgd', 'lbfgs'],
              'hidden_layer_sizes':[(30, 20, 10)],
              'random_state':[2],
              'activation':  ['relu', 'tanh', 'logistic'],
              'max_iter': [400],
              'alpha': [0.0001, 0.001, 0.01],
              "batch_size" : [128],
              "learning_rate_init": [0.002],
              'learning_rate': ['adaptive','constant'],
              "validation_fraction":[0.2],
              }

param = {'activation': ['logistic'], 'alpha': [0.0001], 'batch_size': [128], 'hidden_layer_sizes': (30, 20, 10), 'learning_rate': ['adaptive'],
                                               'learning_rate_init': [0.002], 'max_iter': [maxlen], 'solver': ['sgd'], 'validation_fraction': [validation_fraction]}

clf = GridSearchCV(MLPClassifier(), param, cv=3, verbose=10)
mlp_hist = clf.fit(X_train,y_train)
mlpG_train = round(clf.score(X_train, y_train) * 100, 2)

print("Fim do treinamento MLP")

print("Salvando modelo")
models_directory = 'models'
if not os.path.exists(models_directory):
    os.makedirs(models_directory)

# Salve o modelo no arquivo
model_file_path = os.path.join(models_directory, 'MLPClassifierWithGridSearchCV.pkl')
with open(model_file_path, 'wb') as arquivo:
    pickle.dump(clf, arquivo)


"""#### RNN (LSTM bidirecionais)"""

print("=" * 20)
print("Modelo RNN (LSTM bidirecionais)")

modelLSTM = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_length, output_dim, input_length=maxlen),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)),
    tf.keras.layers.Dense(5, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
modelLSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print('Summary do modelo RNN (LSTM bidirecionais) ')
modelLSTM.summary()
print()
# Função para contar iterações
class BatchCounter(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        self.batch_count = 0

    def on_batch_end(self, batch, logs=None):
        self.batch_count += 1
batch_counter_callback = BatchCounter()


print("Inciando do treinamento")
historyLSTM = modelLSTM.fit(train_padded_seqeunces, y_train, epochs=epochs, validation_split=0.3, callbacks=[early_stopping,tensorboard_callback,batch_counter_callback])
print("Fim do treinamento")
print("Salvando modelo")
modelLSTM.save('models/modelLSTM.keras', save_format='tf')
print(f'Total de iterações modelLSTM: {batch_counter_callback.batch_count}')

"""#### RNN (HAN)"""
print("=" * 20)
print("Modelo RNN (HAN)")


def word_attention(x):
    return Attention()([x, x])

def sentence_attention(x):
    return Attention()([x, x])

document_input = Input(shape=(maxlen,))
word_embedding = Embedding(vocab_length, output_dim, input_length=maxlen)(document_input)
word_lstm = Bidirectional(LSTM(50, return_sequences=True))(word_embedding)
word_attention = word_attention(word_lstm)
sentence_lstm = Bidirectional(LSTM(20, return_sequences=True))(word_attention)
sentence_attention = sentence_attention(sentence_lstm)
doc_lstm = Bidirectional(LSTM(10))(sentence_attention)
dense_layer = Dense(5, activation='relu')(doc_lstm)
dropout_layer = tf.keras.layers.Dropout(0.5)(dense_layer)
output = Dense(1, activation='sigmoid')(dropout_layer)

model_han = tf.keras.Model(inputs=document_input, outputs=output)
model_han.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


print('Summary do modelo RNN (LSTM bidirecionais) ')
print(model_han.summary())
print()

print("Inciando do treinamento")
historyHAN = model_han.fit(train_padded_seqeunces, y_train, epochs=epochs, validation_split=0.2, callbacks=[early_stopping,tensorboard_callback])
print("Fim do treinamento")

print("Salvando modelo")
model_han.save('models/modelHAN.keras', save_format='tf')