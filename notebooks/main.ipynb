{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gyova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay, classification_report, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                False\n",
       "preprocessed_news    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def remover_stop_words(news):\n",
    "    palavras = news.split()\n",
    "    palavras_sem_stop = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "    return ' '.join(palavras_sem_stop)\n",
    "\n",
    "def review_cleaning(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df[\"preprocessed_news\"] = df[\"preprocessed_news\"].apply(remover_stop_words)\n",
    "df[\"preprocessed_news\"] = df[\"preprocessed_news\"].apply(review_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.apply(lambda row: 0 if row.label == 'fake' else 1, axis=1)\n",
    "\n",
    "X_train = df.drop(['label'], axis = 1)\n",
    "Y_train = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(X_train) + 1\n",
    "maxlen=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train['preprocessed_news'].apply(lambda x: x.lower())\n",
    "train_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>',num_words=8000)\n",
    "train_tokenizer.fit_on_texts(X_train.values)\n",
    "train_word_index = train_tokenizer.word_index\n",
    "train_sequences = train_tokenizer.texts_to_sequences(X_train)\n",
    "vocab_length = len(train_word_index) + 1\n",
    "maxlen=8000\n",
    "train_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=8000)\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length 4609, Train sequences 4608, Test sequences 1152 \n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab length {vocab_length}, Train sequences {len(train_sequences)}, Test sequences {len(test_sequences)} ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train,Y_train)\n",
    "lr_classifier_train = round(lr_classifier.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "with open('../models/logisticRegression.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(lr_classifier, arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=1)\n",
    "mlp.fit(X_train,Y_train)\n",
    "mlp_train = round(mlp.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "with open('../models/MLPClassifier.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(mlp, arquivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer perceptron (MLP) Com GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['sgd', 'lbfgs'],\n",
    "              'hidden_layer_sizes':(20, 10,4), \n",
    "              'random_state':[2],\n",
    "              'activation': ['tanh', 'relu'],\n",
    "              'max_iter': [2000000],\n",
    "              \"batch_size\" : [\"auto\"],\n",
    "              \"learning_rate_init\": [0.004],\n",
    "              \"validation_fraction\":[0.1],\n",
    "              \"learning_rate\": ['constant']\n",
    "              }\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, cv=2)\n",
    "epochs_hist = clf.fit(X_train,Y_train)\n",
    "mlpG_train = round(clf.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "\n",
    "with open('../models/MLPClassifierWithGridSearchCV.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(clf, arquivo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeClassifier()\n",
    "decisionTree.fit(X_train,Y_train)\n",
    "decisionTree_train = round(clf.score(X_train, Y_train) * 100, 2)\n",
    "\n",
    "with open('../models/decisionTree.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(decisionTree, arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "output_dim = 64\n",
    "epochs = 100\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "modelSimpleRNN = tf.keras.models.Sequential()\n",
    "modelSimpleRNN.add(tf.keras.layers.Embedding(vocab_length, output_dim, input_length=maxlen))\n",
    "modelSimpleRNN.add(tf.keras.layers.GRU(60, activation='tanh', return_sequences=True))\n",
    "modelSimpleRNN.add(tf.keras.layers.Conv1D(30, 3, activation='relu'))\n",
    "modelSimpleRNN.add(tf.keras.layers.LSTM(30, return_sequences=True))\n",
    "modelSimpleRNN.add(tf.keras.layers.SimpleRNN(3, activation='tanh'))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dropout(0.25))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))\n",
    "modelSimpleRNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "historySimpleRNN = modelSimpleRNN.fit(train_padded_seqeunces, Y_train, epochs=epochs, validation_split=0.3, callbacks=[early_stopping])\n",
    "rnn_train = round(historySimpleRNN.history['accuracy'][-1] * 100, 2)\n",
    "modelSimpleRNN.save('../models/pc/modelo_rnn.keras', save_format='tf')\n",
    "\n",
    "plt.plot(historySimpleRNN.history['loss'])\n",
    "plt.title('Model loss progress during training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend(['Training loss'])\n",
    "plt.plot(historySimpleRNN.history['accuracy'])\n",
    "plt.title('Model accuracy progress during training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training accuracy')\n",
    "plt.legend(['Training accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "output_dim = 64\n",
    "epochs = 100\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "modelSimpleRNN = tf.keras.models.Sequential()\n",
    "modelSimpleRNN.add(tf.keras.layers.Embedding(vocab_length, output_dim, input_length=maxlen))\n",
    "modelSimpleRNN.add(tf.keras.layers.GRU(60, activation='tanh', return_sequences=True))\n",
    "modelSimpleRNN.add(tf.keras.layers.Conv1D(30, 3, activation='relu'))\n",
    "modelSimpleRNN.add(tf.keras.layers.LSTM(30, return_sequences=True))\n",
    "modelSimpleRNN.add(tf.keras.layers.SimpleRNN(3, activation='tanh'))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dropout(0.25))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dense(2, activation='relu'))\n",
    "modelSimpleRNN.add(tf.keras.layers.Dense(units = 1, activation='sigmoid'))\n",
    "modelSimpleRNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "modelSimpleRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelLSTM = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_length, output_dim, input_length=maxlen),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(10)),\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "modelLSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "historyLSTM = modelLSTM.fit(train_padded_seqeunces, Y_train, epochs=epochs, validation_split=0.3, callbacks=[early_stopping])\n",
    "LSTM_train = round(historyLSTM.history['accuracy'][-1] * 100, 2)\n",
    "modelLSTM.save('../models/modelLSTM.keras', save_format='tf')\n",
    "\n",
    "plt.plot(historyLSTM.history['loss'])\n",
    "plt.title('Model loss progress during training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training loss')\n",
    "plt.legend(['Training loss'])\n",
    "plt.plot(historyLSTM.history['accuracy'])\n",
    "plt.title('Model accuracy progress during training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training accuracy')\n",
    "plt.legend(['Training accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_seqeunces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before reshaping: (4608, 8000)\n",
      "Shape after reshaping: (4608, 8000, 1)\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)       [(None, None, 1)]            0         []                            \n",
      "                                                                                                  \n",
      " bidirectional_67 (Bidirect  (None, None, 40)             3520      ['input_67[0][0]']            \n",
      " ional)                                                                                           \n",
      "                                                                                                  \n",
      " attention_66 (Attention)    (None, None, 40)             0         ['bidirectional_67[0][0]',    \n",
      "                                                                     'bidirectional_67[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_64 (TFOpL  (None, None, 40)             0         ['attention_66[0][0]',        \n",
      " ambda)                                                              'bidirectional_67[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.reduce_sum_64 (TFO  (None, 40)                   0         ['tf.math.multiply_64[0][0]'] \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " dense_62 (Dense)            (None, 2)                    82        ['tf.math.reduce_sum_64[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)        (None, 2)                    0         ['dense_62[0][0]']            \n",
      "                                                                                                  \n",
      " input_66 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_63 (Dense)            (None, 1)                    3         ['dropout_30[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3605 (14.08 KB)\n",
      "Trainable params: 3605 (14.08 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      " 5/51 [=>............................] - ETA: 31:51 - loss: 79.7960 - accuracy: 0.6062"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, GRU, LSTM, SimpleRNN, Conv1D, Dense, Dropout, Attention, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "maxlen=1\n",
    "\n",
    "print(\"Shape before reshaping:\", train_padded_seqeunces.shape)\n",
    "x_train_padded_sequences = train_padded_seqeunces[:, :, tf.newaxis]\n",
    "print(\"Shape after reshaping:\", x_train_padded_sequences.shape)\n",
    "\n",
    "document_input = Input(shape=(maxlen,), dtype='float32')\n",
    "word_embedding = Embedding(vocab_length, output_dim, input_length=maxlen)(document_input)\n",
    "word_lstm = Bidirectional(LSTM(50, return_sequences=True))(word_embedding)\n",
    "word_attention = Attention()([word_lstm, word_lstm])\n",
    "word_representation = tf.reduce_sum(word_attention * word_lstm, axis=1)\n",
    "\n",
    "sentence_input = Input(shape=(None, maxlen), dtype='float32')\n",
    "sentence_encoder = Bidirectional(LSTM(20, return_sequences=True))(sentence_input)\n",
    "sentence_attention = Attention()([sentence_encoder, sentence_encoder])\n",
    "sentence_representation = tf.reduce_sum(sentence_attention * sentence_encoder, axis=1)\n",
    "\n",
    "document_representation = Dense(2, activation='relu')(sentence_representation)\n",
    "document_representation = Dropout(0.5)(document_representation)\n",
    "output = Dense(1, activation='sigmoid')(document_representation)\n",
    "\n",
    "\n",
    "# Crie e compile o modelo\n",
    "model_han = Model(inputs=[sentence_input, document_input], outputs=output)\n",
    "model_han.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Imprima o resumo do modelo\n",
    "model_han.summary()\n",
    "\n",
    "historyHAN = model_han.fit([x_train_padded_sequences, x_train_padded_sequences], Y_train, epochs=epochs,batch_size=64, validation_split=0.3, callbacks=[early_stopping])\n",
    "HAN_train = round(historyHAN.history['accuracy'][-1] * 100, 2)\n",
    "model_han.save('../models/modelHAN.keras', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "maxlen=256\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def remover_stop_words(news):\n",
    "    palavras = news.split()\n",
    "    palavras_sem_stop = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "    return ' '.join(palavras_sem_stop)\n",
    "\n",
    "def review_cleaning(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df_test[\"preprocessed_news\"] = df_test[\"preprocessed_news\"].apply(remover_stop_words)\n",
    "df_test[\"preprocessed_news\"] = df_test[\"preprocessed_news\"].apply(review_cleaning)\n",
    "\n",
    "df_test['label'] = df_test.apply(lambda row: 0 if row.label == 'fake' else 1, axis=1)\n",
    "\n",
    "X_test = df_test['preprocessed_news'].apply(lambda x: x.lower())\n",
    "\n",
    "test_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "test_tokenizer.fit_on_texts(X_test.values)\n",
    "test_word_index = test_tokenizer.word_index\n",
    "test_sequences = test_tokenizer.texts_to_sequences(X_test)\n",
    "vocab_length = len(test_word_index) + 1\n",
    "maxlen=256\n",
    "test_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=maxlen, truncating='post')\n",
    "\n",
    "Y_test = df_test['label']\n",
    "\n",
    "y_test_pred = model_han.predict(test_padded_seqeunces)\n",
    "\n",
    "y_test_pred = (y_test_pred > 0.70).astype(int)\n",
    "\n",
    "Y_test = Y_test.astype(int)\n",
    "Y_test = Y_test.ravel()\n",
    "y_test_pred = y_test_pred.ravel()\n",
    "    \n",
    "han_acc = round(accuracy_score(Y_test, y_test_pred), 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "#df_test = pd.read_csv('../data/dataset_0_palavras.csv')\n",
    "#df_test = df_test.drop(columns=['Unnamed: 0', 'ID', 'author',\n",
    "#       'average_word_length', 'category', 'date_publication', 'link',\n",
    "#       'percent_speeling_errors', 'number_words'])\n",
    "\n",
    "#df_test = df_test.rename(columns={'fake_news': 'label'})\n",
    "#df_test = df_test.rename(columns={'text': 'preprocessed_news'})\n",
    "\n",
    "#df_test = df_test.drop(df_test.index[400:], inplace=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=256\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "def remover_stop_words(news):\n",
    "    palavras = news.split()\n",
    "    palavras_sem_stop = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "    return ' '.join(palavras_sem_stop)\n",
    "\n",
    "def review_cleaning(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "df_test[\"preprocessed_news\"] = df_test[\"preprocessed_news\"].apply(remover_stop_words)\n",
    "df_test[\"preprocessed_news\"] = df_test[\"preprocessed_news\"].apply(review_cleaning)\n",
    "\n",
    "df_test['label'] = df_test.apply(lambda row: 0 if row.label == 'fake' else 1, axis=1)\n",
    "\n",
    "X_test = df_test['preprocessed_news'].apply(lambda x: x.lower())\n",
    "\n",
    "test_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "test_tokenizer.fit_on_texts(X_test.values)\n",
    "test_word_index = test_tokenizer.word_index\n",
    "test_sequences = test_tokenizer.texts_to_sequences(X_test)\n",
    "vocab_length = len(test_word_index) + 1\n",
    "maxlen=256\n",
    "test_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=maxlen, truncating='post')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=8000)\n",
    "vectorizer.fit(X_test)\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "Y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/logisticRegression.pkl', 'rb') as arquivo:\n",
    "    lr_classifier = pickle.load(arquivo)\n",
    "with open('../models/MLPClassifier.pkl', 'rb') as arquivo:\n",
    "    mlp = pickle.load(arquivo)\n",
    "with open('../models/MLPClassifierWithGridSearchCV.pkl', 'rb') as arquivo:\n",
    "    mlpG = pickle.load(arquivo)\n",
    "with open('../models/decisionTree.pkl', 'rb') as arquivo:\n",
    "    decisionTree = pickle.load(arquivo)\n",
    "    \n",
    "rnn = load_model('../models/modelo_rnn.keras')\n",
    "modelLSTM = load_model('../models/modelLSTM.keras')\n",
    "model_han = load_model('../models/modelHAN.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lr_classifier.predict(X_test)\n",
    "y_test_pred = (y_test_pred > 0.75)\n",
    "lr_classifier_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = mlp.predict(X_test)\n",
    "y_test_pred = (y_test_pred > 0.75)\n",
    "mlp_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multilayer perceptron (MLP) Com GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = mlpG.predict(X_test)\n",
    "y_test_pred = (y_test_pred > 0.75)\n",
    "mlpG_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = decisionTree.predict(X_test)\n",
    "y_test_pred = (y_test_pred > 0.75)\n",
    "decisionTree_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = rnn.predict(test_padded_seqeunces)\n",
    "y_test_pred = (y_test_pred > 0.70)\n",
    "rnn_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM(BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = modelLSTM.predict(test_padded_seqeunces)\n",
    "y_test_pred = (y_test_pred > 0.70)\n",
    "lstm_acc = round(accuracy_score(Y_test, y_test_pred) * 100, 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model_han.predict(test_padded_seqeunces)\n",
    "\n",
    "y_test_pred = (y_test_pred > 0.70).astype(int)\n",
    "\n",
    "# Ajustar a variável Y_test para garantir que seja do tipo inteiro (se necessário)\n",
    "Y_test = Y_test.astype(int)\n",
    "han_acc = round(accuracy_score(Y_test, y_test_pred), 2)\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_test_pred)\n",
    "print(classification_report(Y_test, y_test_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake','True']).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparando Modelos Diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Logistic Regression',\n",
    "        'Decision Tree',\n",
    "        'MLPClassifier',\n",
    "        'MLPClassifier with grid',\n",
    "        'RNN',\n",
    "        'RNN LSTM(BI)',\n",
    "        'HAN'\n",
    "    ],\n",
    "    'Model Accuracy Score': [\n",
    "        lr_classifier_acc, decisionTree_acc,\n",
    "        mlp_acc, mlpG_acc,rnn_acc,lstm_acc,han_acc\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.sort_values('Model Accuracy Score',ascending=False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_test.drop(df_test.index[1:], inplace=False)\n",
    "df_subset.to_csv('../data/sub_test.csv', index=False)\n",
    "df_subset.preprocessed_news = \"O número de feminicídios aumentou durante o governo Bolsonaro. Os dados do Anuário Brasileiro de Segurança Pública mostraram que em 2018 último ano antes de ele assumir como presidente o Brasil teve um total de 1.229 casos (página 157). Nos dois primeiros anos do governo Bolsonaro o número aumentou para 1.330 em 2019 e 1.354 em 2020. Em 2021 o número caiu para 1.341 mas seguiu acima do patamar de 2018.\"\n",
    "\n",
    "new_row = new_row = pd.DataFrame({'index': [1], 'label': ['0'], 'preprocessed_news': [\"Não pega fogo na floresta amazônica. Isso é falácia – Jair Bolsonaro (PL) presidente e candidato à reeleição em sabatina da RedeTV em 1 de setembro de 2022\"]})\n",
    "df_subset = pd.concat([df_subset, new_row], ignore_index=True)\n",
    "new_row = new_row = pd.DataFrame({'index': [2], 'label': ['1'], 'preprocessed_news': [\"Hoje você leva em média um dia para abrir uma empresa – Jair Bolsonaro (PL) presidente e candidato à reeleição em sabatina da RedeTV em 1 de setembro de 2022\"]})\n",
    "df_subset = pd.concat([df_subset, new_row], ignore_index=True)\n",
    "\n",
    "new_row = new_row = pd.DataFrame({'index': [1], 'label': ['1'], 'preprocessed_news': [\",historia fisico minas marte brasileiro ivair gontijo narra livro participacao missao curiosity agosto curiosity pousava marte maior sofisticado veiculo espacial ser enviado sucesso outro planeta radares permitiram pouso momento critico missao projetados equipe liderada fisico brasileiro ivair gontijo pesquisador jet propulsion laboratory jpl agencia espacial americana nasa novo livro caminho marte acaba ser lancado gontijo conta nao apenas detalhes aventura envolveu projeto lancamento pouso historico curiosity tambem singular trajetoria pessoal criado pequena cidade moema margens rio sao francisco interior minas gerais gontijo decidiu trocar emprego fazenda curso fisica universidade federal minas gerais deixando familiares incredulos so comeco antes chegar nasa ainda fez doutorado escocia estados unidos elon musk divulga design atualizado proximo lancador espacial objetivo livro mostrar estudantes brasileiros geral conseguir algo trabalhar nasa alcance maioria pessoas disposta pagar preco preco alto horas trabalho estudo foco planejamento disse gontijo estado autor intercala narrativa propria trajetoria historia estudos sobre marte segundo apaixonado humanidade ha seculos cientistas precisamos tornar ciencia acessivel emocionante populacao porque nenhum pais vai frente muita ciencia tecnologia procuro mostrar ciencia pode ser tao emocionante gol final copa mundo afirmou conseguir emprego nasa segundo gontijo extremamente dificil alem formacao solida anos estudo intenso preciso armar insistencia astronomica nao primeira vez bati porta nasa consegui entrar primeira vez fiz entrevista ofereceram emprego temporario achei nao valia pena contato gerente missoes avisou precisavam alguem perfil deu pessoas laboratorio ninguem nunca respondeu nao desisti conta dia engenheiro jpl ligou dizendo encontrou curriculo site conferencia precisavam pessoa exatamente formacao fiz entrevista nao deu certo preciso esperar ano conto todos detalhes livro disse fisico missao segundo gontijo curiosity veiculo sofisticado ja construido enviado outro planeta tarefa dificultada tamanho veiculo semelhante carro compacto curiosity pesaria quilos terra possui mastro dois metros altura missao anterior envolvia veiculo menor torno quilos tecnologia toda viabilizar pouso curiosity marte nao existia antes tecnologia utilizamos missoes anteriores airbags nao funcionaria nessa circunstancia tinhamos fazer veiculo descer rodas explicou segundo cientista trabalho missao dessas dimensoes exigente tenso momentos criticos lancamento pouso basta falha milhares possiveis perderem anos trabalho bilhoes dolares processo descida marte ficou conhecido sete minutos terror porque veiculo aproximando planeta mil quilometros hora rota colisao queda livre velocidade precisava ser reduzida zero apenas sete minutos contou antes descida capsula continha curiosity perde energia atmosfera marciana usando escudo frontal calor paraquedas utilizado reduzir ainda velocidade descida final ultimos quatro seis quilometros usamos retrofoguetes explica voo controlado retrofoguetes veiculo colocado chao guindaste seguida afasta nao causar danos equipamento correu tudo melhor esperavamos vi colega sala controle dizendo achamos solo radar realmente acreditei daria certo seguida lembro comemoracoes sala controle muitos gritos lagrimas espetacular lembra radares gontijo nao funcionassem curiosity espatifaria superficie marciana tudo estaria perdido colegas diziam radares nao funcionassem todo resto irrelevante porque veiculo viraria monte ferro velho superficie marte projeto custou us bilhoes entao tensao grande acordo gontijo missao curiosity ja terminou oficialmente veiculo continua fazendo pesquisas superficie marciana onde percorreu pouco quilometros ja encontramos leito seco rio fundo lago rochas sedimentares locais onde havia agua ph neutro pessoa poderia beber operadores nasa recebem dados curiosity diariamente ate hoje apos recepcao dados cientificos sao discutidos fotos sao analisadas engenheiros tecnicos cientistas decidem fazer dia seguinte comandos sao programados enviados marte todos dias satelite orbita planeta satelite repassa dados veiculo solo executa comandos coleta dados tarde marciana transmite novos dados direcao ceu marciano onde passara satelite reenvia terra explicou futuro atualmente gontijo trabalhando proxima missao marte mars enviara planeta vermelho veiculo semelhante curiosity novo veiculo tera conjunto diferente instrumentos descera local distinto ideia coletor amostras disse cientista amostras coletadas veiculo mars serao colocadas roboticamente tubos serao selados hermeticamente deixados superficie planeta segunda missao sera enviada coletar amostras serao colocadas pequeno foguete enviadas orbita marte ali amostras vao ficar girando torno marte terceira missao simples barata saira entao terra encontrara amostras orbita marte trara tudo volta terra necessario porque terra laboratorios sofisticados analisar amostras chegam ocupar predio inteiro nenhum instrumento possa ser enviado atualmente marte mesma capacidade analise objetivos exploracao marte segundo gontijo encontrar vestigios vida planeta cientistas vestigio fossil forma primitiva vida bacteria totalmente extinta ja descoberta historica igual talvez maiores descobertas especie humana todos seres vivos terra mesma origem todos virus seres humanos usamos processo codificacao dna vida formar outro planeta sera usaria processo nao sabemos dia descobrirmos vida terra sera primeira coisa ser estudada sao processos formam vida outro planeta descoberta espetacular\"]})\n",
    "df_subset = pd.concat([df_subset, new_row], ignore_index=True)\n",
    "new_row = new_row = pd.DataFrame({'index': [2], 'label': ['1'], 'preprocessed_news': [\",relator lava jato nega pedido lula ser ouvido advogados lula haviam pedido prestasse novo depoimento defesa apresentou nesta novos documentos sobre propriedade triplex tribunal regional federal regiao nao vai ouvir luiz inacio lula silva antes julgamento apelacao marcado janeiro primeira instancia politico condenado anos meses corrupcao passiva lavagem dinheiro processo lava jato envolvendo triplex advogados lula haviam protocolado dois pedidos lula ouvido antes julgamento recurso setembro outro inicio janeiro decisao relator processos lava jato tribunal joao gebran neto diz eventual deferimento reinterrogatorio passa necessariamente apreciacao alegacoes invalidade daquele prestado perante juizo origem alem disso gebran cita jurisprudencias anteriores negaram pedidos semelhantes ressalta faculdade decidir novo depoimento somente juizo nao reu acusacao apelacao defesa tambem pede inocentado acusacao ocultacao propriedade imovel recebido propina empreiteira oas troca favores petrobras defesa moro nao interesse apurar fatos nesta advogados lula protocolaram apelacao novos documentos segundo defesa reforcariam oas proprietaria triplex decisao proferida dezembro juizo vara execucao titulos distrito federal autos processo determinou penhora apartamento condominio solaris guaruja sp satisfacao divida oas matricula atualizada cartorio imoveis municipio tambem incluida peticao documentos defesa reforca propriedade imovel pertence oas nao lula sentenca juiz sergio moro permite recorra liberdade setembro advogados lula ja haviam ingressado pedido relator processos lava jato desembargador joao gebran neto porem segundo assessoria imprensa defesa pedido nao apreciado motivou novo pedido julgamento recurso apresentado defesa lula sera realizado dia janeiro sede tribunal regional regiao porto alegre justica federal parana tambem determinou bloqueio r milhoes estabelecido dano minimo sequestro apartamento lula tambem bloqueados r mil contas bancarias cerca r milhoes depositados dois planos previdencia privada outros dois reus processo tambem condenados quatro absolvidos movimento terra mst havia anunciado acampamento manifestar apoio lula durante julgamento proibido justica realizar protesto dessa maneira despacho publicado dezembro neto proibiu instalacao acampamento movimento trabalhadores rurais terra mst area parque terrenos vizinhos ate tres dias apos julgamento manifestantes poderao acessar area caso desembargadores decidam manter decisao primeira instancia podem determinar prisao lula caso executada moro curitiba decidir so ira prisao apos todos recursos terem sido esgotados supremo tribunal federal stf determinou reu condenado segunda instancia ja comece cumprir pena prisao recorrendo tribunais superiores assunto porem deve voltar ser discutido ministros ainda nao ha data julgamento inelegibilidade lula vez assunto justica eleitoral lei ficha limpa preve condenado segunda instancia caso caso desembargadores confirmem sentenca nao pode candidatar eventual condenacao si so nao influenciaria possivel candidatura neste caso decisao ficaria conta tribunal superior eleitoral tse pode ser acionado ministerio publico eleitoral mpe algum adversario politico ainda decidir iniciativa propria magistrado\"]})\n",
    "df_subset = pd.concat([df_subset, new_row], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset[\"preprocessed_news\"] = df_subset[\"preprocessed_news\"].apply(remover_stop_words)\n",
    "df_subset[\"preprocessed_news\"] = df_subset[\"preprocessed_news\"].apply(review_cleaning)\n",
    "\n",
    "\n",
    "X_test = df_subset['preprocessed_news'].apply(lambda x: x.lower())\n",
    "\n",
    "test_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "test_tokenizer.fit_on_texts(X_test.values)\n",
    "test_word_index = test_tokenizer.word_index\n",
    "test_sequences = test_tokenizer.texts_to_sequences(X_test)\n",
    "vocab_length = len(test_word_index) + 1\n",
    "maxlen=256\n",
    "test_padded_seqeunces = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=maxlen, truncating='post')\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=8000)\n",
    "vectorizer.fit(X_test)\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "Y_test = df_subset['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = rnn.predict(test_padded_seqeunces)\n",
    "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "print(y_test_pred)\n",
    "\n",
    "\n",
    "y_test_pred = modelLSTM.predict(test_padded_seqeunces)\n",
    "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "print(y_test_pred)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45f1f3757439874319ad1023556b798bdce3a703ab4652de42b8220c6c143c8c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ven': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
